
# Design Pattern: Ordered Parallel Processing with At-Least-Once Guarantee

## 0. Введение: Архитектура внутренней параллельности

При проектировании высоконагруженных систем разработчик часто сталкивается с жестким ограничением инфраструктуры: данные поступают из последовательного источника (очередь сообщения, лог событий, файл), пропускная способность которого ограничена одним потоком (например, **1 Partition = 1 Consumer**).

Чтобы обойти это ограничение без усложнения инфраструктуры (например, без репартиционирования Kafka топика), применяется паттерн **In-App Parallelism**: приложение вычитывает данные одним потоком (**Reader**), но распределяет фактическую обработку между множеством внутренних воркеров (**Worker Pool**).

Выбор архитектуры этого пула зависит от двух критических требований бизнеса:
1.  **Delivery Semantics (Семантика доставки):** Можно ли терять данные при сбоях?
2.  **Ordering Guarantee (Гарантия порядка):** В каком порядке воркеры должны *возвращать* результаты?

### 0.1. Delivery Semantics: Цена надежности

В реальной практике встречаются два полярных подхода:

#### A. At-Most-Once Pool ("Fire-and-Forget")
Стратегия максимальной скорости. Reader подтверждает получение задачи (commit) источника **сразу** при чтении, еще до передачи воркеру.
*   **Сценарии:** Сбор телеметрии (IoT), кликстрим-аналитика, массовые некритичные рассылки.
*   **Цена:** При падении приложения (OOM, редеплой, краш) все задачи, находящиеся "в памяти", теряются безвозвратно.
*   **Сложность:** Низкая.

#### B. At-Least-Once Pool (Стандарт де-факто)
Стратегия гарантированной доставки. Reader подтверждает задачу источнику только **после** того, как воркер успешно завершил обработку.
*   **Сценарии:** Финтех, e-commerce заказы, важные уведомления. Потеря недопустима; лучше обработать дважды (дубликат), чем ни разу.
*   **Сложность:** Высокая. Требует механизмов синхронизации, обработки сбоев (DLQ) и идемпотентности. **Именно этот подход рассматривается в документе.**

### 0.2. Проблема порядка (Ordering Dilemma)

Вторая фундаментальная проблема параллелизма — рассинхронизация входа и выхода.

Представьте, что Reader передал в пул задачи: $1, 2, 3$.
Из-за разной сложности обработки (или сетевых задержек) воркеры могут завершить их в порядке: $2, 3, 1$.

Здесь выделяют два типа требований к порядку:

1.  **Processing Order (Порядок обработки):**
    *   *Partial / Key-based:* Задачи одного пользователя (User ID) должны обрабатываться строго последовательно, но разные пользователи — параллельно.
    *   *Independent:* Задачи абсолютно независимы (ресайз картинок, email-рассылка). Порядок начала обработки не важен.

2.  **Output / Commit Order (Порядок подтверждения):**
    *   Это требование инфраструктуры, а не бизнес-логики. Многие источники (например, Kafka) поддерживают только **Cumulative Acknowledgement** (накопительное подтверждение).
    *   *Проблема:* Мы не можем подтвердить задачу $2$, пока не подтверждена задача $1$, даже если $2$ уже готова.
    *   Это создает необходимость в механизме **Resequencing (Переупорядочивание)**: буфере, который задерживает быстрые результаты ($2, 3$), ожидая завершения медленного первого элемента ($1$), чтобы подтвердить их пачкой в строгом порядке $1 \to 2 \to 3$.

Вот финальный параграф введения, который объединяет все требования и ограничивает скоуп документа.

---

### 0.3. Цель документа

В данном документе мы детально рассмотрим архитектуру **Ordered Parallel Processing with At-Least-Once Guarantee**.

Мы сфокусируемся на самом сложном и распространенном сценарии интеграции:
1.  **At-Least-Once:** Обработка критичных данных, где потери недопустимы.
2.  **Commit Order (Output Ordering):** Работа с источниками, требующими строго последовательного подтверждения (как Log-based брокеры типа **Apache Kafka** или **YDB Topics**).
    *   *Примечание:* Для брокеров очередей (Queue-based), таких как RabbitMQ или SQS, требование Commit Order обычно не актуально, так как они поддерживают выборочное подтверждение (individual ack) и удаление сообщений в любом порядке. Однако, описанный паттерн применим и к ним, если требуется снизить нагрузку на сеть за счет батчинг-коммитов.
3.  **Processing Order (Key-based Preservation):** Сохранение хронологии событий бизнес-сущностей.
    *   Даже если в источнике (например, в партиции Kafka) сообщения упорядочены, наивная передача их в `Unordered Worker Pool` разрушит этот порядок (более новое сообщение может обработаться раньше старого). Мы рассмотрим, как сохранить локальный порядок обработки без потери параллелизма.


## 1. Ключевая терминология

Для понимания проблематики в документе используются следующие архитектурные термины:

*   **Out-of-Order Commits (Нарушение порядка коммитов):** Ситуация, когда задача, поступившая позже (offset 101), завершается успешно и пытается быть подтвержденной раньше, чем задача, поступившая ранее (offset 100). В системах с накопительным подтверждением это недопустимо.
*   **Head-of-Line Blocking (Блокировка головы очереди):** Ситуация, когда обработка или подтверждение большого потока быстрых задач останавливается из-за одной медленной или зависшей задачи в самом начале очереди ("в голове").
*   **Deferred Commit (Отложенный коммит):** Паттерн, при котором физическая отправка подтверждения Источнику откладывается до момента, пока не будет сформирована непрерывная последовательность выполненных задач.
* **The Gap Problem ("Проблема Разрыва"):** Состояние Реестра, при котором задача с оффсетом $N$ еще обрабатывается (или упала с ошибкой и ретраится), а задачи $N+1...N+K$ уже успешно завершены. Эти завершенные задачи образуют "островки успеха" после "дырки" ($N$). В системах с накопительным подтверждением (Kafka) наличие "дырки" блокирует возможность подтвердить ($commit$) все последующие выполненные задачи, заставляя Resequencer удерживать их в памяти.

---

## 2. Архитектура решения: The Resequencer Pattern

Чтобы обеспечить параллельную обработку с сохранением гарантий, необходимо внедрить буфер упорядочивания. Схема потока данных:

`Source -> Dispatcher -> [Worker Pool] -> Result Registry -> Committer`

### 2.1. Компоненты системы

#### A. Dispatcher (Распределитель)
Читает задачи из Источника. Присваивает каждой задаче монотонный ID (или использует Offset). Регистрирует задачу в *Реестре* как `Pending` и отдает свободному воркеру.

#### B. Result Registry (Реестр Упорядочивания)
Структура данных "в памяти" (например, кольцевой буфер или связный список), которая отслеживает статус всех выданных, но еще не подтвержденных задач. Знает "Low Watermark" — границу, до которой все задачи гарантированно выполнены.

#### C. Committer (Фиксатор)
Асинхронный процесс, реализующий логику **Deferred Commit**.
*   Он мониторит Реестр.
*   Как только задача на границе Low Watermark меняет статус на `Done`, он сдвигает границу вправо.
*   Если задача на границе висит в `Pending`, Committer **ждет** (возникает Head-of-Line Blocking на уровне коммита), даже если задачи впереди уже готовы.

---

## 3. Критические технические нюансы

### 3.1. Стратегия обработки ошибок (Error Policies)

В параллельной среде критически важно, чтобы воркер **никогда** не "глотал" задачу молча. Любое завершение (успех или провал) должно быть зафиксировано в Реестре.

#### Классификация ошибок
Воркер должен уметь различать типы сбоев:

1.  **Retryable Errors (Временные сбои):**
    *   *Примеры:* Тайм-аут соединения с БД, 503 Service Unavailable от внешнего API, дэдлок транзакции.
    *   *Действие:* Воркер обязан попытаться повторить выполнение (Retry) с использованием стратегии **Exponential Backoff**. Реестр не обновляется, задача висит как "в работе".

2.  **Non-Retryable Errors (Перманентные сбои):**
    *   *Примеры:* Ошибка валидации JSON, NullPointerException в коде бизнес-логики, нарушение уникальности ключа (если это ошибка логики).
    *   *Действие:* Повторять бессмысленно. Задача должна быть немедленно помечена как завершенная (с ошибкой), чтобы освободить очередь.

#### Dead Letter Queue (DLQ) и "Logging as DLQ"
Что делать с задачей, которая либо превысила лимит Retry, либо является Non-Retryable? Мы не можем просто выбросить её, так как потеряем данные.

*   **Классический DLQ:** Отправка "битого" сообщения в специальное хранилище или топик для ручного разбора.
*   **Logging as DLQ (Логирование как DLQ):**
    *   Если в инфраструктуре есть надежная система сбора логов (ELK, Splunk, Datadog), можно не использовать отдельное хранилище.
    *   Воркер пишет ошибку в `stdout` со специальными тегами (например, `severity=ERROR`, `event_type=POISON_PILL`, `payload=...`).
    *   Система мониторинга триггерит алерт для разработчиков.
    
**Важно:** После отправки в DLQ или логирования, воркер **обязан** сообщить Реестру статус `Success` (или специальный `Skipped`).
*Почему:* С точки зрения механизма коммитов, обработка завершена. Мы не должны блокировать сдвиг Low Watermark из-за одного битого сообщения.

### 3.2. Параллелизм на основе ключа (Key-based Parallelism)

Здесь описаны детали реализации, удовлетворяющей требованию *Processing Order*, а именно:

*   **Когда нужно:** Если обработка задачи $B$ зависит от результата задачи $A$, и они относятся к одной сущности.
    *   *Пример:* $A$="Создать пользователя", $B$="Начислить бонусы пользователю". Если выполнить $B$ раньше $A$, система упадет с ошибкой "Пользователь не найден".
   
**Реализация:**
Вместо общего пула используется массив очередей/каналов.
*   Диспетчер вычисляет: `QueueID = Hash(Task.Key) % WorkerCount`.
*   Это гарантирует, что все события одного пользователя обрабатываются строго последовательно (одним и тем же воркером), но разные пользователи обрабатываются параллельно.
*   **Нюанс:** Это может привести к неравномерной загрузке ("перекос данных"), если один ключ генерирует 90% трафика.

### 3.3. Backpressure (Обратное давлениe)

Поскольку Dispatcher обычно читает быстрее, чем воркеры обрабатывают, Реестр рискует переполниться. Если воркеры застрянут на "тяжелых" задачах, потребление памяти вырастет до OOM (Out Of Memory).

*   **Решение:** Ограничение кол-ва задач "в полете" (Max In-Flight).
*   Когда Реестр заполнен, Dispatcher должен **приостановить** чтение из Источника (активно перестать запрашивать новые батчи), пока воркеры не разгребут завал.

### 3.4. Отзыв задач и Graceful Shutdown

При редеплое или изменении топологии кластера (Scaling), Источник может потребовать перераспределения задач.

1.  Источник посылает сигнал останова.
2.  Dispatcher прекращает выдачу новых задач.
3.  Система ждет (Wait) завершения текущих задач воркеров (или отменяет их через Context, если они Retryable).
4.  Committer делает **финальный Deferred Commit** до последней "дырки".
5.  Все задачи после "дырки" (даже если они уже `Done`) отбрасываются. При перезапуске они будут вычитаны заново. 

### 3.5. Идемпотентность (Idempotency)
Это архитектурная цена за использование семантики *At-Least-Once*. Из-за механизма "сброса" задач после дырок при ребалансировке (пункт 3.4) или сетевых сбоев подтверждения (Ack lost), одна и та же задача гарантированно будет доставлена воркерам более одного раза.

* **Требование:** Бизнес-логика воркера должна быть устойчива к дубликатам.

* **Реализация:** Идемпотентность должна обеспечиваться либо **на уровне инфраструктуры** (транзакции БД, уникальные индексы), либо **на уровне логики**:

* *Deduplication Table:* Хранение ID успешно обработанных сообщений в БД и проверка перед выполнением.

* *Natural Idempotency:* Использование операций типа `UPSERT` вместо `INSERT` или методов `setState(X)`, которые дают один и тот же результат независимо от количества вызовов.

## 4. Резюме (Checklist для реализации)

1.  **Registry:** Реализован ли буфер, запрещающий коммит $N+1$, пока не готов $N$?
2.  **Policies:** Есть ли четкое разделение на Retryable и Non-retryable ошибки?
3.  **Closing Gaps:** Гарантируется ли, что даже при фатальной ошибке ("Poison Pill") Реестр получит сигнал о завершении (после DLQ/Лога), чтобы не остановить очередь навечно?
4.  **Limits:** Есть ли ограничение по памяти/количеству задач для предотвращения OOM?
5.  **Idempotency:** Готова ли бизнес-логика к тому, что при сбоях некоторые задачи будут выполнены повторно (из-за механизма "сброса" задач после дырок)?

## 5. Эталонная реализация (Reference Implementation)
Описанный выше паттерн является индустриальным стандартом для JVM-экосистемы при работе с Apache Kafka.

Можно ознакомиться с документацией библиотеки **Confluent Parallel Consumer**, которая реализует данную архитектуру (Ordered Registry, Offset Map в метаданных, разрешение Gap-проблемы):

* **GitHub Repository:** [https://github.com/confluentinc/parallel-consumer](https://github.com/confluentinc/parallel-consumer)

* *Примечание:* Библиотека реализует расширенную версию паттерна, где состояние "островков" (Gap state) сжимается в BitSet и сохраняется непосредственно в метаданные офсет-коммита Kafka, что позволяет минимизировать дублирование даже при рестарте приложения.
